<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Advanced Architecture Cookbook Toolkit</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <style>
    body {
      font-family: system-ui, -apple-system, BlinkMacSystemFont, "Segoe UI", sans-serif;
      line-height: 1.6;
      max-width: 960px;
      margin: 0 auto;
      padding: 2rem 1.5rem 4rem;
      color: #111827;
      background: #ffffff;
    }
    h1, h2, h3, h4, h5 {
      color: #0f172a;
      line-height: 1.3;
    }
    h1 { font-size: 2rem; margin-top: 0; }
    h2 { margin-top: 2.25rem; }
    h3 { margin-top: 1.75rem; }
    h4 { margin-top: 1.5rem; }

    p { margin: 0.6rem 0 1rem; }

    ul, ol {
      margin: 0.4rem 0 1rem 1.5rem;
    }

    table {
      border-collapse: collapse;
      width: 100%;
      margin: 1.25rem 0;
      font-size: 0.95rem;
    }
    table thead tr {
      background: #e5e7eb;
    }
    table th, table td {
      border: 1px solid #d1d5db;
      padding: 0.5rem 0.6rem;
      vertical-align: top;
      text-align: left;
    }
    table th {
      font-weight: 600;
    }

    code {
      font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace;
      font-size: 0.9em;
    }
    pre {
      background: #0b1120;
      color: #e5e7eb;
      padding: 1rem 1.25rem;
      overflow: auto;
      border-radius: 0.5rem;
      margin: 1.25rem 0;
      font-size: 0.88rem;
    }

    blockquote {
      margin: 1.25rem 0;
      padding: 0.75rem 1rem;
      border-left: 4px solid #1d4ed8;
      background: #eff6ff;
      color: #111827;
      border-radius: 0 0.5rem 0.5rem 0;
    }

    hr {
      border: 0;
      border-top: 1px solid #e5e7eb;
      margin: 2rem 0;
    }

    .note-figure {
      font-size: 0.9rem;
      color: #4b5563;
      display: block;
      margin-top: 0.4rem;
    }
  </style>
</head>
<body>

<h1>Advanced Architecture Cookbook Toolkit</h1>

<h2><span style="color:#00008B">1. Introduction</span></h2>

<p>Fairness in AI systems is shaped not only by data and metrics, but by architectural choices. Decisions about how models are composed, how components interact, and how optimisation objectives are applied can either amplify or constrain unfair outcomes.</p>

<p>The Advanced Architecture Cookbook provides architecture-specific fairness strategies for common AI system types, including recommendation systems, large language models (LLMs), vision models, and multi-modal systems. It moves beyond generic pre-, in-, and post-processing techniques by focusing on where fairness risks arise in system design and how architectural interventions can be applied proportionately.</p>

<p>This toolkit complements:</p>
<ul>
  <li><strong>Fair AI Scrum Toolkit</strong>, which embeds fairness into delivery practices; and</li>
  <li><strong>Organisational Integration &amp; Governance Toolkit</strong>, which defines accountability, documentation, and escalation.</li>
</ul>

<p>This cookbook focuses on how systems are built and the recipes are illustrative and non-prescriptive. They must be adapted to system context, organisational risk appetite, and approved governance decisions, and should always be accompanied by appropriate documentation and monitoring.</p>

<h2><span style="color:#00008B">2. How to Use This Cookbook</span></h2>

<ul>
  <li><strong>Identify your architecture:</strong> Large Language Model (LLM), Recommendation System (RecSys), Vision Model, or Multi-Modal.</li>
  <li><strong>Documentation:</strong> Document every assumption. Undocumented fixes regress in the next release.</li>
</ul>

<h3><span style="color:#00008B">2.1 Inputs Required</span></h3>

<p>Before selecting or adapting any recipe, teams should have:</p>
<ul>
  <li>a clearly defined system scope and intended use</li>
  <li>approved fairness objectives and metrics</li>
  <li>an initial risk classification (e.g. low, medium, high)</li>
  <li>relevant decision gates and escalation thresholds</li>
</ul>

<p>These inputs are typically produced through the processes defined in AI Fair Scrum and Organisational Integration &amp; Governance toolkits.</p>

<h3><span style="color:#00008B">2.2 Selecting and Adapting Recipes</span></h3>

<p>Engineering teams should:</p>
<ul>
  <li>Identify the dominant system architecture(s)</li>
  <li>Select relevant recipes for that architecture</li>
  <li>Adapt implementation details to their technical stack</li>
  <li>Document trade-offs, limitations, and assumptions in Fairness Decision Records (FDRs)</li>
</ul>

<p>Algorithm and metric choices should be treated as governed decisions, not defaults.</p>

<h3><span style="color:#00008B">2.3 Relationship to Governance and Decision Gates</span></h3>

<p>Outputs from this cookbook should feed directly into:</p>
<ul>
  <li>FDRs (decision rationale, evidence, residual risk)</li>
  <li>decision gates (pre-deployment, change approval, monitoring review)</li>
  <li>dashboards and scorecards that surface fairness signals</li>
</ul>

<h3><span style="color:#00008B">2.4 Cross-Cutting Architectural Fairness Principles</span></h3>

<h4><span style="color:#00008B">A. Separation of Concerns</span></h4>

<p>Architectures should clearly separate:</p>
<ul>
  <li>data ingestion and validation</li>
  <li>representation learning</li>
  <li>decision logic (ranking, scoring, generation)</li>
  <li>monitoring and feedback mechanisms</li>
</ul>

<p>This separation supports traceability, targeted mitigation, and independent review.</p>

<h4><span style="color:#00008B">B. Feedback Loops and Fairness Decay</span></h4>

<p>Optimisation on engagement or performance can unintentionally reinforce bias over time. Architectures should:</p>
<ul>
  <li>make feedback loops explicit</li>
  <li>limit self-reinforcement</li>
  <li>enable post-deployment correction</li>
</ul>

<h4><span style="color:#00008B">C. Monitoring by Design</span></h4>

<p>Fairness metrics should be:</p>
<ul>
  <li>computable post-deployment</li>
  <li>disaggregated (including intersectional analysis where required)</li>
  <li>connected to defined decision gates</li>
</ul>

<h2><span style="color:#00008B">3. Recommendation Systems (RecSys) Suite</span></h2>

<h3><span style="color:#00008B">3.1 Common Fairness Issues</span></h3>

<table>
  <thead>
    <tr>
      <th>Category</th>
      <th>Typical Symptom</th>
      <th>Architectural Risk</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Exposure bias</td>
      <td>Certain groups consistently ranked lower</td>
      <td>Feedback loop amplification</td>
    </tr>
    <tr>
      <td>Popularity reinforcement</td>
      <td>&quot;Head&quot; items dominate exposure over time</td>
      <td>Rich-get-richer dynamics</td>
    </tr>
    <tr>
      <td>Cold start disadvantage</td>
      <td>New users or items receive little or no exposure</td>
      <td>Structural invisibility and delayed opportunity</td>
    </tr>
    <tr>
      <td>Eligibility drift</td>
      <td>Intended users or providers never enter candidate generation</td>
      <td>Misalignment with system purpose</td>
    </tr>
    <tr>
      <td>Provider fairness gaps</td>
      <td>Established providers favoured over equally qualified newcomers</td>
      <td>Structural economic inequality</td>
    </tr>
    <tr>
      <td>Calibration gaps <br> (diversity Limitations)</td>
      <td>Lower recommendation accuracy for niche or minority users</td>
      <td>Persistent under-service and alienation</td>
    </tr>
    <tr>
      <td>Position &amp; presentation bias</td>
      <td>Items shown higher or more prominently receive disproportionate attention</td>
      <td>UI-driven distortion of fairness outcomes</td>
    </tr>
  </tbody>
</table>

<h3><span style="color:#00008B">3.2 Frequent Implementation Mistakes</span></h3>

<!-- (No explicit content under 3.2 in the source beyond the heading) -->

<h3><span style="color:#00008B">3.3 Why Recommendation Systems Are Special</span></h3>

<p>Recommendation systems allocate exposure and opportunity, not just predictions. Small differences in ranking can compound over time, creating persistent unfair outcomes through feedback loops. Fairness interventions must therefore address candidate eligibility, ranking logic, and exposure dynamics.</p>

<p>Key characteristics that make fairness in recommendation systems distinct include:</p>
<ul>
  <li><strong>Feedback loops</strong>: user interactions influence future training data.</li>
  <li><strong>Exposure asymmetry</strong>: items ranked higher receive disproportionate attention.</li>
  <li><strong>Cold start dynamics</strong>: new users or items lack interaction history and may remain invisible.</li>
  <li><strong>UI mediation</strong>: presentation order and layout shape behaviour independently of relevance.</li>
</ul>

<p>As a result, fairness interventions must address candidate generation, ranking, exposure, and monitoring, rather than focusing solely on model accuracy.</p>

<h3><span style="color:#00008B">3.4 Fairness Implementation Recipes</span></h3>

<h4><span style="color:#00008B">Recipe R1: Fairness-Aware Candidate Generation</span></h4>

<ul>
  <li><strong>Problem addressed:</strong> Structural exclusion before ranking.</li>
  <li><strong>When to use:</strong> Under-coverage of eligible groups.</li>
  <li><strong>Architectural intervention:</strong> Candidate generation layer.</li>
  <li><strong>Guidance:</strong> Prefer upstream constraints to downstream penalties.</li>
  <li><strong>Trade-offs:</strong> Reduced short-term relevance.</li>
  <li><strong>Evidence required:</strong> Coverage parity; recall by group.</li>
</ul>

<h4><span style="color:#00008B">Recipe R2: Fairness-Constrained Ranking</span></h4>

<ul>
  <li><strong>Problem addressed:</strong> Qualified candidates ranked lower.</li>
  <li><strong>When to use:</strong> Disparities within eligible pools.</li>
  <li><strong>Architectural intervention:</strong> Ranking objective.</li>
  <li><strong>Trade-offs:</strong> Increased optimisation complexity.</li>
  <li><strong>Evidence required:</strong> Equal opportunity; calibration by group.</li>
</ul>

<h4><span style="color:#00008B">Recipe R3: Feedback Loop Dampening</span></h4>

<ul>
  <li><strong>Problem addressed:</strong> Reinforcement of early bias.</li>
  <li><strong>When to use:</strong> Disparities increase over time.</li>
  <li><strong>Architectural intervention:</strong> Exploration or exposure controls.</li>
  <li><strong>Evidence required:</strong> Exposure trends over time.</li>
</ul>

<h3><span style="color:#00008B">3.5 Recipe Summary</span></h3>

<p>The table below summarises common architectural fairness primitives used in recommendation systems. The final column provides illustrative example ranges only, intended to help teams understand the order of magnitude and type of tuning involved.</p>

<table>
  <thead>
    <tr>
      <th>Stage</th>
      <th>Fairness Primitive</th>
      <th>Purpose</th>
      <th>Illustrative Example Values (Non-Normative)</th>
      <th>Governance Considerations</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Data</td>
      <td>Position-bias correction /<br> Inverse Propensity Scoring</td>
      <td>Reduce distortion from historical ranking effects</td>
      <td>Clip Weights &lt; 10.0</td>
      <td>Assumptions and evidence must be documented</td>
    </tr>
    <tr>
      <td>Training</td>
      <td>Representation disentanglement</td>
      <td>Limit encoding of sensitive attributes</td>
      <td>Auxiliary loss weights (Fisher Loss): 0.5 – 1.0</td>
      <td>Trade-offs with performance require explicit approval</td>
    </tr>
    <tr>
      <td>Ranking</td>
      <td>Exposure-aware re-ranking</td>
      <td>Balance user utility and provider exposure</td>
      <td>&lambda; (Fairness): 0.15 – 0.25</td>
      <td>Metric choice governed (e.g. EO vs exposure parity)</td>
    </tr>
    <tr>
      <td>Exploration</td>
      <td>Fairness-aware exploration</td>
      <td>Mitigate cold start and visibility gaps</td>
      <td>&epsilon; (Exploration): 5% – 10%</td>
      <td>Must be monitored and reviewed at decision gates</td>
    </tr>
    <tr>
      <td>Audit</td>
      <td>Inequality and distribution monitoring</td>
      <td>Detect exposure or opportunity concentration</td>
      <td>Gini &lt; 0.4</td>
      <td>Thresholds approved and reviewed periodically</td>
    </tr>
  </tbody>
</table>

<h3><span style="color:#00008B">3.6 Validation Targets and Monitoring Baselines</span></h3>

<p>The following table defines illustrative validation baselines for recommendation systems. These targets support experimentation and governance review and must be evaluated across relevant demographic and intersectional slices (e.g. age × gender × language background).</p>

<table>
  <thead>
    <tr>
      <th>Validation Dimension</th>
      <th>What It Measures</th>
      <th>Example Validation Method</th>
      <th>Baseline Target (Illustrative)</th>
      <th>Governance Notes</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Exposure Equality</strong></td>
      <td>Distribution of exposure across providers or item groups</td>
      <td>Exposure histograms; slice-based audits</td>
      <td>15% of representation ratios</td>
      <td>Thresholds used for ranking must be governance-approved</td>
    </tr>
    <tr>
      <td><strong>User Outcome Parity</strong></td>
      <td>Recommendation quality consistency across user groups</td>
      <td>Slice-level precision / recall</td>
      <td>&le; 0% disparity across key slices</td>
      <td>Must include at least one intersectional slice</td>
    </tr>
    <tr>
      <td><strong>Cold-Start Fairness</strong></td>
      <td>Visibility of new users or items</td>
      <td>Exploration rate analysis</td>
      <td>&ge; 5–10% controlled exploration</td>
      <td>Exploration parameters are policy controls</td>
    </tr>
    <tr>
      <td><strong>Feedback Loop Detection</strong></td>
      <td>Degree of self-reinforcing bias</td>
      <td>Longitudinal exposure tracking</td>
      <td>No monotonic collapse of diversity</td>
      <td>Requires time-series review</td>
    </tr>
    <tr>
      <td><strong>Calibration Consistency</strong></td>
      <td>Accuracy alignment across user segments</td>
      <td>Reliability curves by slice</td>
      <td>Calibration gap &le; 0.05</td>
      <td>Persistent gaps trigger escalation</td>
    </tr>
    <tr>
      <td><strong>Intersectional Coverage</strong></td>
      <td>Multi-attribute testing adequacy</td>
      <td>Slice coverage review</td>
      <td>&ge; 1 high-risk intersection tested</td>
      <td>Data sparsity treated as risk</td>
    </tr>
  </tbody>
</table>

<p>For acceptable thresholds, aim for:</p>
<ul>
  <li>Quality disparity no greater than 10% across major user demographic groups</li>
  <li>Exposure differences across provider groups within 15% of representation ratios</li>
  <li>Diversity metrics improved by at least 30% compared to pure-relevance baselines</li>
  <li>Relevance retention of at least 90% compared to non-fair recommendation versions</li>
</ul>

<blockquote>
  <p><strong>Important:</strong> These examples are not defaults. Any concrete algorithm choice, parameter value, or threshold must be justified, approved through governance decision gates, and documented in a Fairness Decision Record.</p>
</blockquote>

<h2><span style="color:#00008B">4. Large Language Model (LLM) Suite</span></h2>

<p>Large Language Models introduce fairness risks that differ fundamentally from traditional predictive systems. Their
scale, generative capabilities, and sensitivity to prompts mean that bias can emerge through pre-training data, runtime interactions, and emergent behaviours, often in ways that are difficult to anticipate or detect with standard evaluation techniques.</p>

<p>Fairness risks in LLMs frequently manifest at demographic intersections (e.g. age × gender × ethnicity × language
background), even when single-attribute evaluations appear acceptable. This section provides architecture-specific
guidance for identifying, mitigating, and validating fairness risks in LLM-based systems. These issues often persist
even when models are fine-tuned or constrained, requiring layered mitigation and ongoing oversight. Appendix D provides conceptual foundations for LLMs.</p>

<h3><span style="color:#00008B">4.1 Common Fairness Issues</span></h3>

<table>
  <thead>
    <tr>
      <th>Category</th>
      <th>Typical Symptom</th>
      <th>Architectural Risk</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Pre-training bias manifestation</strong></td>
      <td>Stereotyped associations or skewed framing inherited from large web corpora</td>
      <td>Normalisation of historical discrimination</td>
    </tr>
    <tr>
      <td><strong>Prompt sensitivity</strong></td>
      <td>Seemingly neutral prompts (&ldquo;leadership potential&rdquo;, &ldquo;academic excellence&rdquo;) yield different standards across groups</td>
      <td>Hidden policy drift and unfair evaluations</td>
    </tr>
    <tr>
      <td><strong>Emergent sycophancy</strong></td>
      <td>Model agrees with biased user assumptions</td>
      <td>Reinforcement of harmful beliefs</td>
    </tr>
    <tr>
      <td><strong>Inconsistent reasoning</strong></td>
      <td>Different levels of scrutiny or justification for similar content</td>
      <td>Non-deterministic and unequal treatment</td>
    </tr>
    <tr>
      <td><strong>Intersectional blindness</strong></td>
      <td>Harms concentrated at demographic intersections are missed</td>
      <td>False confidence in fairness compliance</td>
    </tr>
    <tr>
      <td><strong>Asymmetric refusals</strong></td>
      <td>Unequal moderation, blocking, or safety responses</td>
      <td>Unequal access to service and opportunity</td>
    </tr>
    <tr>
      <td><strong>Representation disparities</strong></td>
      <td>Minority languages, cultures, or perspectives underrepresented</td>
      <td>Disparate access to service quality; indirect discrimination</td>
    </tr>
    <tr>
      <td><strong>Safety / fairness guardrail gaps</strong></td>
      <td>Subtle or intersectional harms are not detected by filters (e.g. microaggressions, coded or proxy discrimination)</td>
      <td>Persistent unmitigated harm; audit findings; erosion of trust</td>
    </tr>
  </tbody>
</table>

<p>These issues often persist even when models are fine-tuned or constrained, requiring layered mitigation and ongoing oversight.</p>

<blockquote>
  <p><strong>Intersectionality requirement:</strong> For each high-risk or user-facing LLM application, teams must evaluate at least one intersectional slice that reflects real users and risk (e.g. age band × gender × language background). Where data is sparse, uncertainty must be documented and treated as a fairness risk requiring governance review.</p>
</blockquote>

<h3><span style="color:#00008B">4.2 Frequent Implementation Mistakes</span></h3>

<p>Teams commonly undermine LLM fairness by:</p>
<ul>
  <li>Treating prompts as neutral text rather than policy-defining artefacts.</li>
  <li>Relying on single metrics (e.g. toxicity only) and missing tone, stereotyping, or quality disparities.</li>
  <li>Evaluating only average prompts, ignoring prompt sensitivity and adversarial phrasing.</li>
  <li>Assuming safety filters equal fairness controls (they do not catch subtle bias or proxy discrimination).</li>
  <li>Testing only single protected attributes, masking intersectional harm.</li>
  <li>Shipping prompt or policy changes without decision gates.</li>
  <li>Lacking versioned prompt governance, making investigations irreproducible.</li>
  <li>Overfitting to benchmark suites while failing on domain-specific scenarios and end-to-end user journeys, leading to false confidence in fairness performance.</li>
  <li>No decision gate on &quot;prompt + model + policy&quot; changes, with teams shipping prompt updates like copy edits despite their material impact on fairness, safety, and user outcomes.</li>
</ul>

<p>These mistakes commonly result in fairness regressions post-deployment.</p>

<h3><span style="color:#00008B">4.3 Why LLMs Are Special</span></h3>

<p>LLMs require fairness strategies beyond those used for traditional ML systems because:</p>
<ul>
  <li>Pre-training dominates behaviour: Bias is embedded in vast web corpora and cannot be fully removed downstream.</li>
  <li>Generative outputs lack ground truth: Correctness-based metrics are insufficient for fairness evaluation.</li>
  <li>Prompt-driven behaviour: User and system prompts act as policy mechanisms shaping outcomes.</li>
  <li>Emergent capabilities: Instruction-following, in-context learning, and sycophancy appear only at scale.</li>
  <li>Dynamic risk surface: New failure modes emerge as users interact with the system in novel ways.</li>
</ul>

<p>As a result, fairness controls must combine design-time, runtime, and post-deployment interventions, supported by governance and monitoring.</p>


<div class="mermaid">
%%{init: {'theme': 'neutral', 'themeVariables': { 'fontSize': '14px'}}}%%
flowchart TD

    %% Parent node
    IM[LLM Implementation]

    %% First-level nodes
    PT[Pre-training Data Bias]
    FS[Fairness Strategies]
    EB["Emergent Behaviours"]
    EM[Evaluation Metrics]
    FE["Fairness Evaluation\nFramework"]
    IX["Intersectionality\nPrinciple"]

    %% Link parent to first-level nodes
    IM --> PT
    IM --> FS
    IM --> EB
    IM --> EM
    IM --> FE
    IM --> IX

    %% Pre-training Data Bias leaf (single list node)
    PT --> PT_LIST["- Representation Disparities\n- Stereotype Encoding\n- Distributional Bias\n- Historical Discrimination\n- Harmful Content"]

    %% Fairness Strategies leaf
    FS --> PB["Prompt-Based"]
    PB --> PB_LIST["- Fairness Prompting<br/>- Self-Critique Framework<br/>- Scaffolded Generation<br/>- Counterfactual Prompting<br/>- Chain-of-Thought Fairness"]

    FS --> FT["Fine-Tuning"]
    FT --> FT_LIST["- Balanced Fine-tuning Dataset<br/>- Fairness-specific RLHF<br/>- Counterfactual Data<br/>- Augmentation<br/>- Bias-targeted Adapters<br/>- Multi-objective Fine-tuning"]

    FS --> RAG["RAG"]

    %% Emergent Behaviours leaf 
    EB --> EB_LIST["- Instruction Following\n- In-context Learning\n- Sycophancy\n- Jailbreak Vulnerability\n- Hallucination"]
    
    %% Fairness Evaluation Framework
    FE --> FE_LIST["- Red Teaming\n- Counterfactual Evaluation\n- Benchmark Suites\n- Human Evaluation Protocols\n- Multi-dimensional Measurement"]
    
    %% Evaluation Metrics
    EM --> EM_LIST["- Representation Fairness<br/>- Stereotyping Measurement<br/>- Counterfactual Consistency<br/>- Safety Effectiveness<br/>- Capability Preservation"]
	
    %% Intersectionality
    IX --> IX_LIST["- Evaluate combined attributes, e.g. age × gender × language<br/>- Test at least one high-risk intersection<br/>- Document uncertainty where data is sparse<br/>- Treat uncovered intersections as risk"]

    %% Styling
    classDef main fill:#bbf,stroke:#333,stroke-width:1px;
    classDef sec_nodes fill:#ECECFF,stroke:#333,stroke-width:1px;
    classDef children_list fill:#D5E8D4,stroke:#333,stroke-width:1px,text-align:left;
    classDef fs_children fill:#c9e4ff,stroke:#333,stroke-width:1px,text-align:left;
 	
    class IM main;
    class EB,EM,FS,PT,FE,IX sec_nodes;
    class EM_LIST,PT_LIST,FE_LIST,EB_LIST,FT_LIST,PB_LIST,IX_LIST children_list;
    class PB,FT,RAG fs_children;
</div>



<p class="note-figure"><strong>Figure 1.</strong> LLM fairness architecture illustrating how pre-training bias, fairness strategies,
emergent behaviours, guardrails, evaluation, and intersectionality interact across the system lifecycle. This diagram supports transparency, shared understanding, and architectural reasoning for fairness implementation.</p>

<h3><span style="color:#00008B">4.4 Recipe Summary</span></h3>

<p>The recipes in the table below illustrate LLM-specific primitives for mitigating fairness risks. Parameter ranges are illustrative starting points for experimentation and must be validated, documented, and approved through governance before becoming release conditions.</p>

<table>
  <thead>
    <tr>
      <th>Stage</th>
      <th>Primitive</th>
      <th>Purpose</th>
      <th>Illustrative Baseline Range (for experimentation)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Corpus</strong></td>
      <td>Dynamic thematic balancing</td>
      <td>Reduce topic–demographic skew in training data</td>
      <td>Balance ratio: 0.08 – 0.12</td>
    </tr>
    <tr>
      <td><strong>Pre-train / continue-train</strong></td>
      <td>Opposite-corpus blending</td>
      <td>Inject counter-stereotypical and counterfactual patterns</td>
      <td>10 – 20% mixture</td>
    </tr>
    <tr>
      <td><strong>Fine-tune</strong></td>
      <td>Fairness-weighted RLHF, RLEF &amp; RLCF (RLHF-F, RLEF-F, RLCF-F)</td>
      <td>Reward bias-aware and consistent completions</td>
      <td>Fairness reward weight: 0.6 – 1.0</td>
    </tr>
    <tr>
      <td><strong>Prompting</strong></td>
      <td>Structured fairness prompts with self-check</td>
      <td>Reduce prompt sensitivity and enforce consistent evaluation criteria</td>
      <td>Self-check on 10 – 30% of calls</td>
    </tr>
    <tr>
      <td><strong>Decode</strong></td>
      <td>Self re-rank with diversity</td>
      <td>Select fairest candidate output among multiple generations</td>
      <td>k = 4 – 8, rerank weight 0.3 – 0.6</td>
    </tr>
    <tr>
      <td><strong>Post-output</strong></td>
      <td>Bias filtering and escalation rules</td>
      <td>Catch residual, subtle, or coded harms</td>
      <td>Context-dependent</td>
    </tr>
    <tr>
      <td><strong>Monitoring</strong></td>
      <td>Counterfactual prompt probes (including intersectional variants)</td>
      <td>Detect drift and emergent fairness harms over time</td>
      <td>Probe refresh monthly / quarterly</td>
    </tr>
  </tbody>
</table>

<p>Alignment-based fine-tuning methods such as Reinforcement Learning from Expert Feedback (RLEF) and Reinforcement
Learning from Constitutional Feedback (RLCF) may be used to encode fairness preferences in LLM behaviour.</p>

<p>These approaches shape model outputs based on evaluative feedback rather than data distribution alone:</p>
<ul>
  <li><strong>RLHF</strong> supports scalable fairness alignment by incorporating human preference signals into model optimisation, but requires careful validation and governance oversight to avoid reinforcing existing biases or majority norms.</li>
  <li><strong>RLEF</strong> supports high-fidelity fairness alignment in high-risk domains through expert judgement and is preferred where human oversight and accountability are required.</li>
  <li><strong>RLCF</strong> operationalises organisational fairness principles through an explicit, versioned constitution aligned with governance policy.</li>
</ul>

<p>The choice of alignment method, reward structure, and evaluation criteria must be justified based on system risk, domain sensitivity, and regulatory exposure.</p>

<blockquote>
  <p><strong>Governance Note:</strong><br />
  Any parameter, rule, or artefact that functions as a <em>policy or safety threshold</em> must be treated as a governance-controlled setting and recorded in a Fairness Decision Record (FDR).</p>
  <p>This includes, but is not limited to:</p>
  <ul>
    <li>Reranking weights and bias filter thresholds</li>
    <li>Reward functions and scoring rubrics used in alignment-based fine-tuning (e.g. RLEF, RLCF)</li>
    <li>Constitutional principles or policy prompts used in RLCF</li>
    <li>Escalation or refusal rules embedded in prompts or post-processing</li>
  </ul>
  <p>Such settings must be reviewed and approved at the appropriate decision gate, with evidence of stability across relevant demographic and intersectional slices, and with residual risks explicitly documented and accepted.</p>
</blockquote>

<p>These primitives are intended to be combined, iterated, and evaluated experimentally before being promoted to release conditions. Teams are expected to:</p>
<ul>
  <li>Validate effects across relevant demographic and intersectional subgroups, prioritising those that reflect real users
    and known risk.</li>
  <li>Document observed trade-offs, limitations, and uncertainty as part of ongoing Fairness Decision Records.</li>
  <li>Treat baseline ranges as starting points, not targets, and avoid operationalising them without governance approval.</li>
</ul>

<h4><span style="color:#00008B">4.4.1 Fairness-Oriented Prompt Template (Illustrative)</span></h4>

<p>The following fairness-oriented prompt demonstrates how the “Structured fairness prompts with self-check” primitive can be implemented in practice. This example is non-prescriptive and must be evaluated and approved through governance before production use.</p>

<div style="border:1px solid #1e3a8a; border-left:6px solid #1e3a8a; padding:14px; border-radius:8px; background:#f8fafc">

  <h4>Example: Fairness-Oriented Prompt Template (Illustrative)</h4>

  <p><strong>System (recommended):</strong></p>

  <p>You are a fairness-aware assistant. Follow the policy:</p>

  <ul>
    <li>Apply consistent standards across users and groups.</li>
    <li>Do not use protected characteristics (or proxies) as justification.</li>
    <li>If information is missing, state assumptions and request clarifications rather than guessing.</li>
    <li>If the task is high-stakes (employment, housing, credit, education, healthcare), be conservative: explain uncertainty and recommend human review.</li>
  </ul>

  <p><strong>Developer / Task context:</strong></p>

  <p>You are assisting with <strong>{TASK}</strong> for <strong>{DOMAIN}</strong>.<br />
  Decision context: <strong>{DECISION_CONTEXT}</strong> (e.g., screening support, ranking support, drafting feedback, summarisation).<br />
  Allowed inputs: <strong>{ALLOWED_INPUTS}</strong>.<br />
  Disallowed inputs: protected characteristics and proxies (unless explicitly permitted for fairness auditing): <strong>{DISALLOWED_INPUTS}</strong>.<br />
  Success criteria: <strong>{SUCCESS_CRITERIA}</strong>.<br />
  Fairness criteria (local policy): <strong>{FAIRNESS_CRITERIA}</strong> (e.g., consistency, equal opportunity intention, non-discrimination, accessibility).</p>

  <p><strong>User / Instance:</strong></p>

  <p>Here is the case to process:</p>
  <ul>
    <li>Candidate / item description: <strong>{INPUT_TEXT}</strong></li>
    <li>Role / objective (if relevant): <strong>{ROLE_OBJECTIVE}</strong></li>
    <li>Constraints: <strong>{CONSTRAINTS}</strong> (e.g., must-have requirements)</li>
    <li>Evidence sources available: <strong>{EVIDENCE}</strong> (e.g., CV, structured fields, test results)</li>
  </ul>

  <h5>Step 1 — Produce the primary output (task result)</h5>

  <p>Provide <strong>{OUTPUT_TYPE}</strong> in the requested format, using only allowed inputs.</p>

  <h5>Step 2 — Fairness self-check (required for {SELF_CHECK_RATE}% of calls or for high-risk use cases)</h5>

  <p>Answer briefly:</p>
  <ol>
    <li><strong>Consistency check:</strong> Would you produce materially different output if the same evidence were written in a different dialect / style / register? If yes, adjust.</li>
    <li><strong>Proxy check:</strong> Did you rely on proxies for protected traits (name, accent cues, nationality cues, gap explanations, etc.)? If yes, remove/rewrite.</li>
    <li><strong>Intersectional check:</strong> Name <strong>at least one plausible intersectional risk</strong> relevant to the domain (e.g., age × gender; language background × nationality; caregiving history × age). State whether the output could disproportionately disadvantage that slice.</li>
    <li><strong>Uncertainty disclosure:</strong> What is uncertain due to missing data? What human review is recommended?</li>
  </ol>

  <h5>Step 3 — Counterfactual probe (for monitoring, not shown to end users)</h5>

  <p>Create <strong>two counterfactual variants</strong> of the input that keep qualifications constant but vary <em>non-job-relevant</em> surface cues (e.g., name origin / language style / career-break framing).
  State whether the outcome changes. If it does, flag as <strong>potential bias drift</strong> and trigger: <strong>FDR update + decision gate review</strong>.</p>

  <p><strong>Output format for monitoring log</strong></p>

  <ul>
    <li>Outcome A: {…}</li>
    <li>Outcome B: {…}</li>
    <li>Difference: {none / small / material}</li>
    <li>Action: {none / log only / escalate}</li>
  </ul>

</div>

<h3><span style="color:#00008B">4.5 Example Validation Targets and Monitoring Baselines (LLMs)</span></h3>

<p>The table below provides illustrative validation targets for Large Language Models. LLM fairness validation requires multidimensional evaluation that reflects their generative nature. The targets below are baselines, not guarantees, and must be assessed across relevant demographic and intersectional groups.</p>

<table>
  <thead>
    <tr>
      <th>Validation Dimension</th>
      <th>What It Measures</th>
      <th>Example Validation Method</th>
      <th>Baseline Target (Illustrative)</th>
      <th>Governance Notes</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Counterfactual Consistency</strong></td>
      <td>Stability of outputs when only demographic attributes change</td>
      <td>Counterfactual prompt variants; semantic similarity scoring; human review</td>
      <td>&ge; 90% semantic equivalence</td>
      <td>Required for high-impact systems; must include &ge;1 intersectional slice</td>
    </tr>
    <tr>
      <td><strong>Stereotype Reduction</strong></td>
      <td>Reduction of harmful or stereotyped language</td>
      <td>Bias benchmarks; red-teaming; human evaluation</td>
      <td>&ge; 80% reduction vs baseline</td>
      <td>Residual stereotypes must be documented, not ignored</td>
    </tr>
    <tr>
      <td><strong>Refusal / Safety Symmetry</strong></td>
      <td>Consistency of refusals and moderation across groups</td>
      <td>Paired demographic prompt tests</td>
      <td>&le; 10% variance across slices</td>
      <td>Covers both over- and under-enforcement</td>
    </tr>
    <tr>
      <td><strong>Harm Leakage Rate</strong></td>
      <td>Subtle harms passing through filters (e.g. microaggressions, coded language)</td>
      <td>Adversarial prompting; qualitative review</td>
      <td>&ge; 95% detection / mitigation</td>
      <td>Longitudinal tracking required</td>
    </tr>
    <tr>
      <td><strong>Capability Preservation</strong></td>
      <td>Retention of useful system behaviour after mitigation</td>
      <td>Task success; preference scores; latency</td>
      <td>&ge; 90% of baseline capability</td>
      <td>Fairness gains must justify losses</td>
    </tr>
    <tr>
      <td><strong>Intersectional Coverage</strong></td>
      <td>Adequacy of multi-attribute testing</td>
      <td>Subset coverage review</td>
      <td>&ge; 1 high-risk intersection tested</td>
      <td>Sparse data = documented risk</td>
    </tr>
  </tbody>
</table>

<p>Validation results must feed into decision gates, be documented in FDRs, and trigger escalation where thresholds are
breached (Figure 2).</p>

<div class="mermaid">
%%{init: {'theme': 'neutral', 'themeVariables': { 'fontSize': '14px'}}}%%

flowchart LR

    %% Evaluation framework
    FE[Fairness Evaluation\nFramework]

    FE --> FE_LIST["- Red Teaming\n- Counterfactual Evaluation\n- Benchmark Suites\n- Human Evaluation Protocols\n- Multi-dimensional Measurement"]

    %% Decision gates
    DG1[Gate 1:\nPre-deployment Approval]
    DG2[Gate 2:\nRelease or\nMaterial Change]
    DG3[Gate 3:\nPost-deployment\nMonitoring Review]
    DG4[Gate 4:\nDecommissioning or\nMajor Repurposing]

    %% Mapping evaluation to gates
    FE_LIST --> DG1
    FE_LIST --> DG2
    FE_LIST --> DG3
    FE_LIST --> DG4

    %% Outcomes
    DG1 --> O1["- Approve deployment\n- Require mitigation\n- Escalate or block"]

    DG2 --> O2["- Approve change\n- Require re-testing\n- Escalate"]

    DG3 --> O3["- Accept risk temporarily\n- Mandate mitigation\n- Suspend or roll back"]

    DG4 --> O4["- Decommission system\n- Replace or restrict use\n- Capture organisational learning"]

    %% Styling
    classDef main fill:#bbf, stroke:#333,stroke-width:1px;
    classDef actions fill:#ECECFF,stroke:#333,stroke-width:1px, text-align:left;
    classDef gates fill:#D5E8D4,stroke:#333,stroke-width:1px;
    classDef outcome fill:#c9e4ff,stroke:#333,stroke-width:1px, text-align:left;
    
    class FE main;
    class FE_LIST actions;
    class DG1,DG2,DG3,DG4 gates;
    class O1,O2,O3,O4 outcome
    
</div>

<p class="note-figure"><strong>Figure 2.</strong> Mapping between fairness evaluation activities and governance decision gates for LLM
systems. This artefact supports transparent decision-making, consistent escalation, and quality control across deployment, change management, monitoring, and decommissioning.</p>

<h2><span style="color:#00008B">5. Vision Models Suite</span></h2>

<p>Vision models differ fundamentally from other ML architectures because fairness risks can be introduced before
learning begins and may remain hidden inside internal representations. Bias may enter through image capture pipelines including sensors, lighting, framing, background context, and annotation practices shaping what the model is able to perceive. These upstream effects are often unevenly distributed across demographic groups and environmental conditions. Latent visual representations can encode sensitive attributes such as skin tone, age, or gender even when they are not explicitly labelled or intended for use. Once embedded, these signals may leak into downstream decisions.</p>

<p>Context sensitivity further amplifies fairness risks. Performance often degrades when environmental factors (e.g. low light, camera quality, background) interact with demographic attributes, creating intersectional harms that remain invisible in aggregate metrics. As a result, fairness in vision systems must address data capture, representation learning, and evaluation not only output metrics or post-hoc thresholds. This section provides architecture-aware recipes to support that end.</p>

<h3><span style="color:#00008B">5.1 Common Fairness Issues</span></h3>

<p>Bias can enter before data reaches the model, through sensors, lighting, and annotation practices. Latent representations may encode sensitive attributes even when they are not explicit.</p>

<table>
  <thead>
    <tr>
      <th>Category</th>
      <th>Symptom</th>
      <th>Impact</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Skin-tone accuracy gap</strong></td>
      <td>Darker skin tones misdetected or misclassified more frequently</td>
      <td>Safety risk; regulatory exposure</td>
    </tr>
    <tr>
      <td><strong>Background leakage</strong></td>
      <td>Model relies on contextual cues (e.g. kitchen, workplace) rather than subject</td>
      <td>Systemic stereotyping</td>
    </tr>
    <tr>
      <td><strong>Attribute leakage</strong></td>
      <td>Latent embeddings encode gender, ethnicity, or age</td>
      <td>Privacy violation, proxy discrimination</td>
    </tr>
    <tr>
      <td><strong>Sensor bias</strong></td>
      <td>Camera settings favour certain tones or contrasts</td>
      <td>Exclusion of specific user groups</td>
    </tr>
    <tr>
      <td><strong>Intersectional degradation</strong></td>
      <td>Error rates spike for combined attributes (e.g. darker skin × low light × older age)</td>
      <td>Hidden harm not visible in aggregated metrics</td>
    </tr>
  </tbody>
</table>

<blockquote>
  <p><strong>Environmental robustness note:</strong><br />
  Vision models are highly sensitive to capture conditions such as lighting, camera quality, angle, and background. Performance disparities often emerge outside controlled environments and disproportionately affect certain demographic groups.</p>

  <p><strong>Intersectional impact note:</strong><br />
  Fairness failures in vision systems frequently compound at demographic intersections (e.g. skin tone × gender × age × environment), producing harms that are not detectable through single-attribute or aggregated evaluation.</p>
</blockquote>

<h3><span style="color:#00008B">5.2 Frequent Implementation Mistakes</span></h3>

<ol>
  <li><strong>Reporting only aggregate accuracy or mAP:</strong><br />
     Overall performance metrics mask substantial disparities across demographic groups and environmental conditions. Slice-level and intersectional evaluation is required to surface real-world harms.</li>
  <li><strong>Naïve or unrealistic data augmentation:</strong><br />
     Synthetic augmentation that oversaturates colours, distorts textures, or ignores real lighting physics can introduce artefacts that reduce validity and create false confidence in fairness improvements.</li>
  <li><strong>Using a single decision threshold across all groups:</strong><br />
     Applying uniform thresholds without analysing group- or context-specific error profiles often results in unequal false-positive or false-negative rates across demographic intersections.</li>
  <li><strong>Ignoring capture pipeline variability:</strong><br />
     Failing to test across different cameras, lenses, resolutions, lighting conditions, and backgrounds leads to fairness regressions when models move from controlled environments to real-world deployment.</li>
  <li><strong>Evaluating attributes in isolation rather than intersections:</strong><br />
     Testing skin tone, gender, or age separately misses compounded failure modes (e.g. older women with darker skin under low-light conditions), where the most severe disparities frequently occur.</li>
</ol>

<h3><span style="color:#00008B">5.3 Why Vision Models are Special</span></h3>

<p>Vision and multi-modal systems introduce fairness risks that differ fundamentally from text or tabular models due to how visual signals are captured, represented, and fused with other modalities. Key architectural characteristics driving these risks include:</p>

<ul>
  <li><strong>Upstream bias before learning</strong><br />
      Visual data pipelines embed bias through sensors, lighting, framing, annotation practices, and cultural context before any learning occurs.</li>
  <li><strong>Latent attribute leakage in representations</strong><br />
      Visual embeddings can encode sensitive attributes (e.g. skin tone, age, gender, disability cues) that persist through downstream tasks, even when those attributes are not explicitly used.</li>
  <li><strong>Context and environment sensitivity</strong><br />
      Performance varies across lighting, background, camera quality, and setting, often creating intersectional degradation when environmental conditions interact with demographic attributes.</li>
  <li><strong>High-saliency illusions</strong><br />
      Explanations highlight wrong areas, fooling auditors.</li>
</ul>

<p>As a result, fairness interventions for vision must address data capture, representation learning and intersectional evaluation, rather than relying on output metrics alone.</p>

<h3><span style="color:#00008B">5.4 Recipe Summary</span></h3>

<p>The recipes below illustrate architecture-specific fairness primitives for vision and multi-modal systems. Each recipe targets a distinct layer of the system lifecycle, from visual representation learning to cross-modal fusion and runtime monitoring. Parameter ranges are illustrative baselines for experimentation only. Any parameter that functions as a policy, safety, or release threshold must be validated across relevant demographic and intersectional slices, documented, and approved through governance before production use.</p>

<table>
  <thead>
    <tr>
      <th>Stage</th>
      <th>Primitive</th>
      <th>Purpose</th>
      <th>Illustrative Baseline Range</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Augmentation</strong></td>
      <td>Style-transfer equalisation</td>
      <td>Balance lighting and texture across groups</td>
      <td>Strength <strong>0.2 – 0.4</strong></td>
    </tr>
    <tr>
      <td><strong>Audit</strong></td>
      <td>Prototype leakage probe</td>
      <td>Quantify demographic information leakage</td>
      <td>Leak score &lt; 0.02</td>
    </tr>
    <tr>
      <td><strong>Remediation</strong></td>
      <td>Layer reinitialisation</td>
      <td>Remove leakage-prone representation blocks</td>
      <td>Context-dependent</td>
    </tr>
    <tr>
      <td><strong>Thresholding</strong></td>
      <td>Group-aware thresholds</td>
      <td>Reduce asymmetric FP/FN rates</td>
      <td>Evaluated per disaggregated groups</td>
    </tr>
    <tr>
      <td><strong>Monitoring</strong></td>
      <td>Slice-aware drift detection</td>
      <td>Detect post-deployment degradation</td>
      <td>Continuous</td>
    </tr>
  </tbody>
</table>

<blockquote>
  <p>Any primitive that materially affects fairness outcomes (e.g. leakage thresholds or reinitialisation criteria) must be reviewed via a governance decision gate and documented in a Fairness Decision Record (FDR).</p>
</blockquote>

<h3><span style="color:#00008B">5.5 Example Validation Targets and Monitoring Baselines</span></h3>

<p>This section provides illustrative validation baselines for vision systems, derived from sample implementations and training material. These values are intended to support early experimentation, benchmarking, and structured review. They do not constitute release thresholds and must not be promoted to policy controls without formal governance approval.</p>

<p>All validation must be conducted across relevant demographic and intersectional slices, including environmental conditions (e.g. lighting, camera quality, background).</p>

<table>
  <thead>
    <tr>
      <th>Validation Dimension</th>
      <th>Example Metric</th>
      <th>Illustrative Baseline Target</th>
      <th>Governance Notes</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Representation leakage</strong></td>
      <td>Leak probe AUC</td>
      <td>&le; 0.52</td>
      <td>Leakage signals require architectural review</td>
    </tr>
    <tr>
      <td><strong>Leak severity</strong></td>
      <td>Leak score</td>
      <td>&lt;0.02</td>
      <td>Context dependant leakage threshold</td>
    </tr>
    <tr>
      <td><strong>Accuracy parity</strong></td>
      <td>mIoU or accuracy gap</td>
      <td>&le; 2–5%</td>
      <td>Slice-level only, aggregate metrics insufficient</td>
    </tr>
    <tr>
      <td><strong>Capture robustness</strong></td>
      <td>Performance under capture variation</td>
      <td>No group-specific collapse</td>
      <td>Field conditions required</td>
    </tr>
    <tr>
      <td><strong>Intersectional coverage</strong></td>
      <td>Error rate at intersection</td>
      <td>No spike beyond single-attribute gaps</td>
      <td>Sparse data treated as risk</td>
    </tr>
  </tbody>
</table>

<p>These metrics capture complementary aspects of leakage (detectability vs magnitude).</p>

<blockquote>
  <p><strong>Governance note:</strong></p>
  <p>Any metric that is used to justify deployment, retraining, or threshold changes must be reviewed through a fairness decision gate and recorded in a Fairness Decision Record (FDR). Where data is sparse, uncertainty must be documented and treated as a risk rather than ignored.</p>
</blockquote>

<h2><span style="color:#00008B">6. Multi-Modal Systems Suite</span></h2>

<p>Vision and Multi-Modal Systems Need Specialised Fairness Approaches. Standard fairness methods that worked for text or speech failed for visual and multimodal components. Architecture-specific strategies addressing representation learning and modality fusion are essential.
Severe fairness issues can emerge from interactions between modalities rather than within individual channels. Addressing these cross-modal effects required specialised fusion approaches beyond single-modality fairness methods. Intersectional Analysis can unveil hidden patterns that affect specific demographic intersections. Only explicit intersectional evaluation can address these critical patterns.</p>

<div class="mermaid">
%%{init: {'theme': 'neutral', 'themeVariables': { 'fontSize': '14px'}}}%%

flowchart LR
    %% Inputs
    V[Visual Input\nImages / Video]
    T["Text Input"]
    A[Audio Input]

    %% Modality-specific encoders
    VE["Vision Encoder\n(CNN / ViT)"]
    TE[Text Encoder]
    AE[Audio Encoder]

    %% Representation checks
    VR["Representation Audit\n(Leakage / Slice Tests)"]
    TR[Text Bias Checks]
    AR[Audio Bias Checks]

    %% Fusion
    F["Fusion Layer\n(Late / Hierarchical)"]
    C[Confidence Signals\n per Modality]

    %% Routing
    R["Fairness-Aware Routing\n& Fallback Logic"]

    %% Output
    O[Decision / Prediction]

    %% Monitoring & Governance
    M["Monitoring & Evaluation\nIntersectional Slices"]
    G[Decision Gate\n+ FDR Update]

    %% Flows
    V --> VE --> VR --> F
    T --> TE --> TR --> F
    A --> AE --> AR --> F

    F --> C --> R --> O
    O --> M --> G

    %% Styling
    classDef input fill:#bbf, stroke:#333,stroke-width:1px;
    classDef model_flow fill:#ECECFF,stroke:#333,stroke-width:1px;
    classDef fusion fill:#c9e4ff,stroke:#333,stroke-width:1px;
    classDef gates fill:#D5E8D4,stroke:#333,stroke-width:1px;

    class V,T,A input;
    class VE,TE,AE,VR,TR,AR model_flow;
    class F fusion;
    class G gates;

</div>

<p class="note-figure"><strong>Figure 3.</strong> Vision and multi-modal fairness architecture showing where representation bias, fusion effects, and intersectional harms can emerge, and how evaluation and governance intervene.</p>

<h3><span style="color:#00008B">6.1 Common Fairness Issues</span></h3>

<table>
  <thead>
    <tr>
      <th>Fairness Issue</th>
      <th>Typical Symptom</th>
      <th>Risk</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Bias transfer</td>
      <td>One modality contaminates others</td>
      <td>Compounded harm</td>
    </tr>
    <tr>
      <td>Modality dominance</td>
      <td>One signal overwhelms others</td>
      <td>Narrow representation</td>
    </tr>
    <tr>
      <td>Missing modality Fallback</td>
      <td>Model fails when one stream unavailable (e.g., low-light video)</td>
      <td>Unequal performance</td>
    </tr>
    <tr>
      <td>Inconsistent Predictions</td>
      <td>Conflicting modality cues yield erratic output</td>
      <td>User confusion</td>
    </tr>
  </tbody>
</table>

<h3><span style="color:#00008B">6.2 Frequent Mistakes</span></h3>

<p>Common implementation pitfalls include:</p>

<ol>
  <li><strong>Annotation Inconsistency:</strong> data ara labelled with different standards across demographics.</li>
  <li><strong>Modality Conflict:</strong> different modalities suggesting contradictory outputs with varying implications across demographics.</li>
  <li><strong>Ignoring Contextual Sensitivity:</strong> vision performance variability across environmental conditions.</li>
  <li><strong>Representation Complexity:</strong> nuance demographic encodings difficult to detect and tackle.</li>
  <li><strong>Recalibration:</strong> skipping recalibration after modality dropout shifts operating point.</li>
  <li><strong>Neglect Confidence-based Routing:</strong> forces low-quality signals into decisions.</li>
</ol>

<h3><span style="color:#00008B">6.3 Why Multi-Modal Systems are Special</span></h3>

<p>Multi-modal systems combine heterogeneous inputs which failure modes differ. Bias can propagate or compound across modalities, making coordination essential.</p>

<ul>
  <li><strong>Cross-modal bias transfer and amplification</strong><br />
      In multi-modal systems, bias in one modality (e.g. vision) can propagate to others (e.g. text or decision outputs), creating compounded or emergent harms not visible in single-modality evaluation.</li>
  <li><strong>Heterogeneous failure modes across modalities</strong><br />
      Different modalities fail differently (e.g. vision under low light, audio with accents), and naïve fusion can result in unequal treatment when one modality dominates or drops out.</li>
  <li><strong>Misleading explainability signals</strong><br />
      Saliency maps and cross-modal explanations may highlight intuitive features while masking deeper representational or fusion-level bias, giving false confidence to auditors.</li>
</ul>

<p>As a result, fairness interventions for vision and multi-modal systems must address data capture, representation learning, modality fusion, and intersectional evaluation, rather than relying on output metrics alone.</p>

<h3><span style="color:#00008B">6.4 Architecture Selection Guide (Vision &amp; Multi-Modal Systems)</span></h3>

<p>Fairness outcomes in vision and multi-modal systems are strongly shaped by architectural decisions made before training begins. Choices around encoders, fusion strategies, and representation sharing determine how bias is learned, amplified, or mitigated. This section provides architecture-level guidance to help teams select designs that support fair outcomes.</p>

<h3><span style="color:#00008B">6.4.1 Architecture Selection Principles</span></h3>

<p>When designing multi-modal systems, teams should prefer architectures that:</p>

<ul>
  <li><strong>Preserve modality separability early:</strong><br />
      independent encoders allow modality-specific bias analysis and remediation before fusion.</li>
  <li><strong>Support controlled fusion:</strong><br />
      late or hierarchical fusion reduces unintended bias amplification compared to early concatenation.</li>
  <li><strong>Expose confidence signals:</strong><br />
      architectures that surface per-modality confidence enable fairness-aware routing and fallback.</li>
  <li><strong>Enable interpretability:</strong><br />
      designs compatible with modality-specific attribution tools improve accountability and auditability.</li>
</ul>

<h4><span style="color:#00008B">6.4.2 Fusion Strategy Patterns</span></h4>

<p>Recommended fusion patterns include:</p>

<ul>
  <li><strong>Late fusion with consistency constraints:</strong><br />
      aligns outputs while allowing independent modality evaluation.</li>
  <li><strong>Confidence-weighted fusion:</strong><br />
      reduces reliance on degraded or biased modalities in specific contexts.</li>
  <li><strong>Fallback-aware routing:</strong><br />
      explicitly handles missing or low-quality modalities to avoid unequal treatment.</li>
</ul>

<h4><span style="color:#00008B">6.4.3 Architecture Decision Table (Fairness Implications)</span></h4>

<table>
  <thead>
    <tr>
      <th>Design Choice</th>
      <th>Fairness Implication</th>
      <th>When Appropriate</th>
      <th>Governance Notes</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>CNN encoder</strong></td>
      <td>Sensitive to capture conditions</td>
      <td>Controlled environments</td>
      <td>Requires robustness testing</td>
    </tr>
    <tr>
      <td><strong>ViT encoder</strong></td>
      <td>Global bias amplification risk</td>
      <td>Diverse, balanced datasets</td>
      <td>Data audit required</td>
    </tr>
    <tr>
      <td><strong>Early fusion</strong></td>
      <td>Bias amplification</td>
      <td>Low-risk exploratory systems</td>
      <td>Avoid for high-risk use</td>
    </tr>
    <tr>
      <td><strong>Late fusion</strong></td>
      <td>Better bias isolation</td>
      <td>Most production systems</td>
      <td>Preferred default</td>
    </tr>
    <tr>
      <td><strong>Shared encoders</strong></td>
      <td>Leakage across attributes</td>
      <td>Homogeneous tasks</td>
      <td>Requires leakage audits</td>
    </tr>
    <tr>
      <td><strong>Separate encoders</strong></td>
      <td>Clear accountability</td>
      <td>High-risk or regulated domains</td>
      <td>Strongly recommended</td>
    </tr>
  </tbody>
</table>

<blockquote>
  <p>If fairness cannot be explained <em>per modality</em>, the architecture is likely too coupled to be governed safely.</p>
</blockquote>

<h3><span style="color:#00008B">6.5 Architecture-Specific Fairness Recipes</span></h3>

<p>This section provides architecture-specific fairness recipes for multi-modal systems. Each recipe targets a distinct technical leverage point where fairness risks commonly arise in modality fusion, routing, and runtime behaviour. These recipes translate high-level fairness principles into concrete implementation patterns that engineering teams can apply within agile delivery cycles.</p>

<p>The recipes below are not prescriptive controls. They are starting points for experimentation and must be validated across relevant demographic and intersectional slices. Any recipe that materially affects fairness outcomes must be reviewed and approved through governance before being treated as a release condition. Appendix E provides an illustrative script example for a fairness blocking decision for a multimodal model.</p>

<h4><span style="color:#00008B">6.5.1 Modality-Specific Fairness Controls</span></h4>

<p>Fairness controls for multimodal models prevent bias from being introduced or amplified within individual modalities before fusion.</p>

<p><strong>Implementation pattern:</strong></p>
<ul>
  <li>Maintain separate encoders per modality where feasible.</li>
  <li>Apply modality-specific fairness diagnostics (e.g. slice evaluation, representation leakage probes).</li>
  <li>Address detected bias at the modality level before system-level optimisation.</li>
</ul>

<p><strong>Fairness risks addressed:</strong></p>
<ul>
  <li>Latent demographic encoding</li>
  <li>Uneven performance across modalities</li>
  <li>Hidden modality-specific bias masked by fusion</li>
</ul>

<p><strong>Governance note:</strong><br />
Changes to modality encoders or representation constraints must trigger an update to the Fairness Decision Record (FDR).</p>

<h4><span style="color:#00008B">6.5.2 Controlled Fusion Strategies</span></h4>

<p><strong>Purpose:</strong><br />
Reduce unintended cross-modal bias amplification during fusion.</p>

<p><strong>Implementation pattern:</strong></p>
<ul>
  <li>Prefer late or hierarchical fusion over early concatenation.</li>
  <li>Introduce explicit cross-modal consistency constraints at fusion layers.</li>
  <li>Monitor disagreement patterns between modalities as fairness signals.</li>
</ul>

<p><strong>Fairness risks addressed:</strong></p>
<ul>
  <li>Bias transfer from one modality to others</li>
  <li>Emergent bias patterns at fusion points</li>
  <li>Dominant modality effects</li>
</ul>

<p><strong>Governance note:</strong><br />
Fusion strategy changes constitute a material system change and must be reviewed at a formal decision gate.</p>

<h4><span style="color:#00008B">6.5.3 Confidence-Based Routing and Fallback Logic</span></h4>

<p><strong>Purpose:</strong><br />
Avoid unequal treatment when one modality is missing, degraded, or unreliable.</p>

<p><strong>Implementation pattern:</strong></p>
<ul>
  <li>Surface <strong>per-modality confidence or quality signals</strong> at runtime.</li>
  <li>Route decisions away from low-confidence modalities.</li>
  <li>Implement explicit fallback logic rather than silent degradation.</li>
</ul>

<p><strong>Fairness risks addressed:</strong></p>
<ul>
  <li>Missing-modality disadvantage</li>
  <li>Context-dependent exclusion (e.g. low light, noisy audio)</li>
  <li>Intersectional harm when degradation disproportionately affects certain groups</li>
</ul>

<p><strong>Governance note:</strong><br />
Fallback rules function as policy thresholds and must be documented, justified, and approved via governance.</p>

<h4><span style="color:#00008B">6.5.4 Runtime Monitoring for Cross-Modal Fairness</span></h4>

<p>Run tme monitoring help detect fairness regressions and emergent harms after deployment.</p>

<p><strong>Implementation pattern:</strong></p>
<ul>
  <li>Track performance and disagreement metrics per modality and at fusion outputs.</li>
  <li>Monitor <strong>intersectional slices</strong> (e.g. demographic attributes × modality availability).</li>
  <li>Use monitoring signals to trigger FDR updates and decision gates.</li>
</ul>

<p><strong>Fairness risks addressed:</strong></p>
<ul>
  <li>Post-deployment bias drift</li>
  <li>Compounded harms across modalities</li>
  <li>Undetected intersectional failures</li>
</ul>

<p><strong>Governance note:</strong><br />
Monitoring dashboards provide <strong>decision triggers</strong>, not decisions. Escalation pathways must be predefined.</p>

<h4><span style="color:#00008B">6.5.5 Summary of Core Multi-Modal Fairness Primitives</span></h4>

<p>The table below summarises the core architectural primitives used to address fairness risks in multi-modal systems. Where applicable, illustrative parameter ranges are shown for traceability. These values are starting points for experimentation and must not be treated as fixed fairness guarantees.</p>

<table>
  <thead>
    <tr>
      <th>Stage</th>
      <th>Primitive</th>
      <th>Purpose</th>
      <th>Illustrative Parameter Reference</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Encoding</strong></td>
      <td>Modality-specific fairness checks</td>
      <td>Isolate and remediate bias before fusion</td>
      <td>Context-dependent (slice-based)</td>
    </tr>
    <tr>
      <td><strong>Fusion</strong></td>
      <td>Cross-modal consistency controls</td>
      <td>Prevent bias amplification</td>
      <td>Loss weight <strong>1.0 – 2.0</strong></td>
    </tr>
    <tr>
      <td><strong>Runtime</strong></td>
      <td>Confidence-based routing</td>
      <td>Handle degraded or missing modalities fairly</td>
      <td>Confidence &ge; <strong>0.65</strong></td>
    </tr>
    <tr>
      <td><strong>Monitoring</strong></td>
      <td>Intersectional slice tracking</td>
      <td>Detect compound harms over time</td>
      <td>Agreement &ge; <strong>0.92</strong><br />Bias migration &le; <strong>1%</strong></td>
    </tr>
  </tbody>
</table>

<p>Together, these primitives operationalise fairness for multi-modal systems at the architectural level. Rather than treating fairness as a post-hoc metric correction, they embed controls at encoding, fusion, runtime, and monitoring stages where bias is most likely to emerge or compound.</p>

<p>By pairing illustrative technical ranges with explicit governance ownership, teams are enabled to experiment responsibly while ensuring that material fairness impacts are reviewed, documented, and approved before release. This structure supports iterative improvement without eroding accountability, particularly for systems operating across heterogeneous inputs and high-risk demographic intersections.</p>

<p><strong>Governance reminder:</strong><br />
Any architectural primitive that influences fairness outcomes (e.g. fusion constraints, routing logic, confidence thresholds) must be treated as a <strong>governance-controlled setting</strong>, recorded in a Fairness Decision Record (FDR), and reviewed at the appropriate fairness decision gate.</p>

<h3><span style="color:#00008B">6.6 Validation Targets and Monitoring Baselines</span></h3>

<p>Multi-modal systems introduce fairness risks through cross-modal interactions, modality dominance, and failure under missing or degraded inputs. The baselines below are illustrative reference points drawn from sample materials and training exercises. They are intended to support structured experimentation and monitoring, not to define compliance thresholds.</p>

<p>Validation must explicitly assess cross-modal bias transfer and intersectional impacts, particularly where one modality disproportionately affects certain demographic groups.</p>

<table>
  <thead>
    <tr>
      <th>Validation Dimension</th>
      <th>What It Measures</th>
      <th>Example Validation Method</th>
      <th>Baseline Target (Illustrative)</th>
      <th>Governance Notes</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Cross-Modal Consistency</strong></td>
      <td>Agreement between modalities</td>
      <td>Agreement scoring across inputs</td>
      <td>&ge; <strong>92% agreement</strong></td>
      <td>Disagreement patterns must be analysed</td>
    </tr>
    <tr>
      <td><strong>Bias Migration</strong></td>
      <td>Transfer of bias between modalities</td>
      <td>Modality-isolated testing</td>
      <td>&le; <strong>1% increase</strong> vs unimodal baseline</td>
      <td>Migration requires architectural review</td>
    </tr>
    <tr>
      <td><strong>Fallback Fairness</strong></td>
      <td>Performance when one modality is missing</td>
      <td>Dropout simulations</td>
      <td>&le; <strong>10% degradation</strong> across groups</td>
      <td>Fallback logic is a policy decision</td>
    </tr>
    <tr>
      <td><strong>Modality dominance</strong></td>
      <td>Over-reliance on a single modality</td>
      <td>Attribution analysis</td>
      <td>No single contributes &gt;70% of decision weight</td>
      <td>Thresholds require governance approval</td>
    </tr>
    <tr>
      <td><strong>Compound Harm Detection</strong></td>
      <td>Intersectional amplification across modalities</td>
      <td>Red-teaming; counterfactual tests</td>
      <td>No systematic amplification</td>
      <td>High-risk intersections prioritised</td>
    </tr>
    <tr>
      <td><strong>Intersectional Amplification</strong></td>
      <td>Compound harms across modalities</td>
      <td>Red-teaming, counterfactuals</td>
      <td>No systematic amplification</td>
      <td>High-risk intersections prioritised</td>
    </tr>
  </tbody>
</table>

<p><strong>Monitoring guidance:</strong></p>

<blockquote>
  <p>Dashboard signals indicating modality imbalance, bias migration, or intersectional amplification must trigger an update to the relevant Fairness Decision Record and, where material, escalation through a fairness decision gate.</p>
</blockquote>

<h2><span style="color:#00008B">7. Cheat Sheet: Mapping Failures to Architecture Recipes</span></h2>

<p>This section provides a quick-reference guide to help teams map observed fairness failures to the most appropriate architectural fairness primitive in the Advanced Architecture Cookbook.</p>

<p>It is intended for:</p>
<ul>
  <li>Incident triage</li>
  <li>Design reviews</li>
  <li>Sprint planning</li>
  <li>Post-evaluation remediation discussions</li>
</ul>

<p>This cheat sheet does not replace diagnosis or governance review. Any remediation applied using these recipes must be validated, documented, and approved through the appropriate fairness decision gate.</p>

<h3><span style="color:#00008B">7.1 Mapping Fairness Signals to Recipes</span></h3>

<table>
  <thead>
    <tr>
      <th>If This Signal <br />or Metric Fails</th>
      <th>Use Recipe <br /> or Primitive</th>
      <th>Cookbook Section</th>
      <th>Intervention Rationale</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Toxicity, stereotyping, or tone disparities (LLM)</strong></td>
      <td>Fairness-weighted RLHF / RLEF / RLCF</td>
      <td>4.4 – Fine-tune</td>
      <td>Alignment-based fine-tuning directly shapes generative behaviour and is most effective for output-level fairness issues.</td>
    </tr>
    <tr>
      <td><strong>Prompt sensitivity or inconsistent reasoning (LLM)</strong></td>
      <td>Structured fairness prompts with self-check</td>
      <td>4.4 – Prompting</td>
      <td>Prompts function as policy artefacts; structured self-checks reduce hidden standards and prompt-driven bias.</td>
    </tr>
    <tr>
      <td><strong>Counterfactual inconsistency across demographics (LLM)</strong></td>
      <td>Counterfactual prompt probes + monitoring</td>
      <td>4.4 – Monitoring</td>
      <td>Reveals bias that accuracy or toxicity metrics miss; supports drift detection and escalation.</td>
    </tr>
    <tr>
      <td><strong>Representation leakage in vision embeddings</strong></td>
      <td>Prototype leakage probe → Layer reinitialisation</td>
      <td>5.4 – Audit / Remediation</td>
      <td>Leakage originates in representations; correcting outputs alone cannot remove encoded demographic signals.</td>
    </tr>
    <tr>
      <td><strong>Accuracy gap tied to lighting, texture, or camera conditions (Vision)</strong></td>
      <td>Style-transfer equalisation</td>
      <td>5.4 – Augmentation</td>
      <td>Forces learning of invariant structure rather than demographic-correlated capture artefacts.</td>
    </tr>
    <tr>
      <td><strong>Asymmetric FP/FN rates across demographic groups (Vision)</strong></td>
      <td>Group-aware thresholds</td>
      <td>5.4 – Thresholding</td>
      <td>Thresholds control decision asymmetry and must be adjusted per group or context to reduce harm.</td>
    </tr>
    <tr>
      <td><strong>Post-deployment performance degradation for specific groups (Vision)</strong></td>
      <td>Slice-aware drift detection</td>
      <td>5.4 – Monitoring</td>
      <td>Detects fairness decay caused by changing environments or user populations.</td>
    </tr>
    <tr>
      <td><strong>Bias propagates from vision into text or decision output (Multi-modal)</strong></td>
      <td>Modality-specific fairness controls</td>
      <td>6.5.1 – Encoding</td>
      <td>Bias must be addressed before fusion; downstream fixes cannot isolate modality-specific causes.</td>
    </tr>
    <tr>
      <td><strong>One modality dominates decisions (Multi-modal)</strong></td>
      <td>Controlled / late fusion strategies</td>
      <td>6.5.2 – Fusion</td>
      <td>Late or hierarchical fusion limits unintended bias amplification across modalities.</td>
    </tr>
    <tr>
      <td><strong>Unequal performance when a modality is missing or degraded</strong></td>
      <td>Confidence-based routing and fallback logic</td>
      <td>6.5.3 – Runtime</td>
      <td>Runtime routing is safer than retraining and prevents exclusion under real-world conditions.</td>
    </tr>
    <tr>
      <td><strong>Intersectional harm emerges only when modalities interact</strong></td>
      <td>Intersectional slice monitoring across modalities</td>
      <td>6.5.4 + 6.6 – Monitoring</td>
      <td>Cross-modal intersectional failures are invisible without explicit slice-based monitoring.</td>
    </tr>
  </tbody>
</table>

<h3><span style="color:#00008B">7.2 How to Use This Cheat Sheet</span></h3>

<ol>
  <li><strong>Identify the failing signal</strong>
    <ul>
      <li>Metric breach, audit finding, user complaint, or red-team outcome.</li>
    </ul>
  </li>
  <li><strong>Locate the architectural layer</strong>
    <ul>
      <li>Encoding, fusion, runtime, prompting, fine-tuning, or monitoring.</li>
    </ul>
  </li>
  <li><strong>Select the corresponding recipe</strong>
    <ul>
      <li>Use this table to choose the <em>lowest-level effective intervention</em>.</li>
    </ul>
  </li>
  <li><strong>Validate before release</strong>
    <ul>
      <li>Test across relevant demographic and intersectional slices.</li>
    </ul>
  </li>
  <li><strong>Document and govern</strong>
    <ul>
      <li>Record the change, evidence, and residual risk in a Fairness Decision Record (FDR).</li>
      <li>Escalate where the change affects policy, thresholds, or user outcomes.</li>
    </ul>
  </li>
</ol>

<h3><span style="color:#00008B">7.3 Governance Reminder</span></h3>

<blockquote>
  <p>This cheat sheet supports decision-making, not automatic fixes.
  Any recipe that materially affects fairness outcomes — including thresholds, routing logic, fusion strategies, prompts, or alignment objectives — must be treated as a <strong>governance-controlled setting</strong>, reviewed at the appropriate decision gate, and documented in an FDR.</p>
</blockquote>

<h2><span style="color:#00008B">8. Reusable Architectural Fairness Primitives</span></h2>

<p>Certain fairness mechanisms recur across architectures, regardless of whether the system is a recommendation engine, LLM, vision model, or multi-modal system. These <strong>reusable architectural fairness primitives</strong> represent <em>design-level building blocks</em> that teams can adapt to their technical stack and risk context.</p>

<p>They are <strong>not fixed algorithms</strong> and do not imply default parameter choices. Their correct use depends on system purpose, risk classification, and approved governance decisions.</p>

<h3><span style="color:#00008B">8.1 Core Reusable Primitives</span></h3>

<ul>
  <li><strong>Group-aware reweighting</strong><br />
    Adjusts sample influence during training or optimisation to counter structural underrepresentation or historical skew. Commonly applied in ranking, fine-tuning, and representation learning stages.</li>
  <li><strong>Slice-aware evaluation harnesses</strong><br />
    Systematically evaluates performance across demographic and intersectional slices, including counterfactual or swapped-attribute variants. Enables detection of hidden harms masked by aggregate metrics.</li>
  <li><strong>Feedback-loop dampening</strong><br />
    Limits self-reinforcing bias over time by constraining optimisation feedback (e.g. exposure caps, exploration budgets, decay factors). Particularly critical in systems that learn from user interaction.</li>
  <li><strong>Confidence-based routing and fallback</strong><br />
    Uses uncertainty or quality signals to route decisions away from unreliable components (e.g. degraded modalities, low-confidence predictions), reducing exclusion and uneven treatment under real-world conditions.</li>
  <li><strong>Bias-sensitive early stopping</strong><br />
    Monitors fairness metrics alongside performance during training and halts optimisation when fairness improvements plateau or regress, preventing overfitting that worsens disparities.</li>
</ul>

<h3><span style="color:#00008B">8.2 How These Primitives Should Be Used</span></h3>

<p>These primitives should be treated as <strong>architectural options</strong>, not default controls. Teams are expected to:</p>

<ul>
  <li>justify <em>why</em> a primitive is appropriate for their system,</li>
  <li>validate its effect across relevant demographic and intersectional slices,</li>
  <li>document trade-offs, uncertainty, and residual risk,</li>
  <li>and integrate the decision into an approved <strong>Fairness Decision Record (FDR)</strong>.</li>
</ul>

<h3><span style="color:#00008B">8.3 Governance Reminder</span></h3>

<p>Any reuse of these primitives that materially affects fairness outcomes including weighting schemes, routing thresholds, evaluation definitions, or stopping criteria must be treated as a governance-controlled setting, reviewed at the appropriate decision gate, and recorded in an FDR. These primitives are intended to support consistency and reuse across teams while preserving local accountability, traceability, and regulatory alignment.</p>

<h2><span style="color:#00008B">9. Closing Guidance</span></h2>

<p>This Advanced Architecture Cookbook is intended as a practical foundation, not a fixed compliance standard. The recipes, primitives, and illustrative parameter ranges provided here are designed to help teams reason about fairness at the architectural level, identify appropriate intervention points, and implement proportionate mitigations within their own technical and organisational contexts.</p>

<p>No recipe, metric, or threshold in this toolkit should be treated as a default production control. Any architectural change that materially affects fairness outcomes must be validated across relevant demographic and intersectional slices, documented in a Fairness Decision Record (FDR), and reviewed through the appropriate governance decision gate.</p>

<p>The cookbook is adaptable and teams are expected to extend, refine, or replace individual techniques as systems evolve, risks change, and regulatory expectations develop. Used correctly, this toolkit supports consistent, transparent, and accountable fairness implementation without constraining innovation or engineering judgement.</p>

<script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
<script>
  mermaid.initialize({
    startOnLoad: true,
    securityLevel: "loose"  // lets <br/> etc. work inside labels
  });
</script>
</body>
</html>
